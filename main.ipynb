{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "np.random.seed(2019)\n",
    "\n",
    "log_path = datetime.now().strftime('./logs/%Y-%m-%d-%H-%M-%S.log')\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_data(dp):\n",
    "    x = []\n",
    "    for fn in sorted(os.listdir(dp), key=lambda y: int(y[:-4])):\n",
    "        with open('{dp}{fn}'.format(dp=dp, fn=fn), 'r') as f:\n",
    "            x.append(f.read())\n",
    "    return x\n",
    "\n",
    "x_tr_pos = np.array(load_data('./dataset/train/pos/'), dtype=np.str)\n",
    "x_tr_neg = np.array(load_data('./dataset/train/neg/'), dtype=np.str)\n",
    "x_tr = np.concatenate((x_tr_pos, x_tr_neg), axis=0)\n",
    "y_tr = np.concatenate((np.ones_like(x_tr_pos, dtype=np.float64), np.zeros_like(x_tr_neg, dtype=np.float64)), axis=0)\n",
    "x_ts = np.array(load_data('./dataset/test/'), dtype=np.str)\n",
    "\n",
    "del x_tr_pos\n",
    "del x_tr_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocessor(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace('<br /><br />', ' ')\n",
    "    s = s.replace('-', ' ')\n",
    "    s = s.replace('/', ' ')\n",
    "    for ws in string.whitespace:\n",
    "        s.replace(ws, ' ')\n",
    "    s = s.translate(s.maketrans('', '', string.punctuation))\n",
    "    s = s.translate(s.maketrans('', '', string.digits))\n",
    "    s = ''.join(filter(lambda x: x in string.printable, s))\n",
    "    return s\n",
    "\n",
    "def tokenizer(s):\n",
    "    wl = WordNetLemmatizer()\n",
    "    st = SnowballStemmer('english', ignore_stopwords=True)\n",
    "    ts = word_tokenize(s, 'english')\n",
    "    ts = list(filter(lambda x: x not in stopwords.words('english'), ts))\n",
    "    ts = list(map(lambda x: wl.lemmatize(x), ts))\n",
    "    ts = list(map(lambda x: st.stem(x), ts))\n",
    "    return ts\n",
    "\n",
    "token_pattern = r'\\w+|[%s]' % string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tf_idf_transform(x_tr, x_ts):\n",
    "    tf_idf = TfidfVectorizer(token_pattern=token_pattern,\n",
    "                             ngram_range=(1, 3))\n",
    "    x_tr = tf_idf.fit_transform(x_tr)\n",
    "    x_ts = tf_idf.transform(x_ts)\n",
    "    return x_tr, x_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def count_transform(x_tr, x_ts):\n",
    "    cnt = CountVectorizer(token_pattern=token_pattern,\n",
    "                          ngram_range=(1, 3),\n",
    "                          binary=True)\n",
    "    x_tr = cnt.fit_transform(x_tr)\n",
    "    x_ts = cnt.transform(x_ts)\n",
    "    return x_tr, x_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "\n",
    "def k_fold_cross_validation(k, cl, x_tr, y_tr):\n",
    "    cv_s = []\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    for i, (tr_idx, cv_idx) in enumerate(kf.split(x_tr)):\n",
    "        x_cnt_tr, x_cnt_ts = count_transform(x_tr[tr_idx], x_tr[cv_idx])\n",
    "\n",
    "        cl.fit(x_cnt_tr, y_tr[tr_idx])\n",
    "        cv_s.append(f1_score(y_tr[cv_idx], cl.predict(x_cnt_ts)))\n",
    "\n",
    "        logger.info('KFold {} CV Score: {}'.format(i, cv_s[-1]))\n",
    "    logger.info('KFold Mean CV Score: {}'.format(sum(cv_s) / k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(cl, x_tr, y_tr, x_ts, fn):\n",
    "    x_tr, x_ts = count_transform(x_tr, x_ts)\n",
    "    cl.fit(x_tr, y_tr)\n",
    "    with open('./results/{fn}.csv'.format(fn=fn), 'w') as f:\n",
    "        f.write('Id,Category\\n')\n",
    "        prd = cl.predict(x_ts)\n",
    "        for i, y_i in enumerate(prd):\n",
    "            f.write('{i},{y_i}\\n'.format(i=i, y_i=int(y_i)))\n",
    "    return prd, cl.predict(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KFold 0 CV Score: 0.8950715421303658\n",
      "KFold 1 CV Score: 0.8957403651115617\n",
      "KFold 2 CV Score: 0.9057560975609756\n",
      "KFold 3 CV Score: 0.9042298483639266\n",
      "KFold 4 CV Score: 0.8960601861017621\n",
      "KFold Mean CV Score: 0.8993716078537183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cl_lr = LogisticRegression(solver='lbfgs', n_jobs=-1)\n",
    "k_fold_cross_validation(5, clone(cl_lr), x_tr, y_tr)\n",
    "prd_ts_lr, prd_tr_lr = predict_test(cl_lr, x_tr, y_tr, x_ts, 'logistic_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/comp551/lib/python3.7/site-packages/sklearn/svm/base.py:931: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "KFold 0 CV Score: 0.8979179300586214\n",
      "KFold 1 CV Score: 0.8982248520710059\n",
      "KFold 2 CV Score: 0.9068029885961464\n",
      "KFold 3 CV Score: 0.8955462352706212\n",
      "KFold 4 CV Score: 0.8904438224710116\n",
      "KFold Mean CV Score: 0.8977871656934813\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "cl_ls = LinearSVC()\n",
    "k_fold_cross_validation(5, clone(cl_ls), x_tr, y_tr)\n",
    "prd_ts_ls, prd_tr_ls = predict_test(cl_ls, x_tr, y_tr, x_ts, 'linear_svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KFold 0 CV Score: 0.9239086904285142\n",
      "KFold 1 CV Score: 0.9231993633107839\n",
      "KFold 2 CV Score: 0.9208949260886935\n",
      "KFold 3 CV Score: 0.9150456530369193\n",
      "KFold 4 CV Score: 0.9196179864703541\n",
      "KFold Mean CV Score: 0.920533323867053\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from nbsvm import NBSVM\n",
    "\n",
    "cl_nb_svm = NBSVM()\n",
    "k_fold_cross_validation(5, clone(cl_nb_svm), x_tr, y_tr)\n",
    "prd_ts_nb_svm, prd_tr_nb_svm = predict_test(cl_nb_svm, x_tr, y_tr, x_ts, 'nb_svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KFold 0 CV Score: 0.8547968885047538\n",
      "KFold 1 CV Score: 0.8905139991642289\n",
      "KFold 2 CV Score: 0.8729351969504447\n",
      "KFold 3 CV Score: 0.8771345272803\n",
      "KFold 4 CV Score: 0.8791932496398437\n",
      "KFold Mean CV Score: 0.8749147723079143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "cl_bnb = BernoulliNB()\n",
    "k_fold_cross_validation(5, clone(cl_bnb), x_tr, y_tr)\n",
    "prd_ts_bnb, prd_tr_bnb = predict_test(cl_nb_svm, x_tr, y_tr, x_ts, 'bernoulli_nb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: The logic of CV in this stacking has some problems.\n",
    "\n",
    "x_stk_tr = np.concatenate((prd_tr_lr.reshape((-1, 1)),\n",
    "                           prd_tr_ls.reshape((-1, 1)),\n",
    "                           prd_tr_bnb.reshape((-1, 1))), axis=1)\n",
    "x_stk_ts = np.concatenate((prd_ts_lr.reshape((-1, 1)),\n",
    "                           prd_ts_ls.reshape((-1, 1)),\n",
    "                           prd_ts_bnb.reshape((-1, 1))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n",
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(x_stk_tr.shape)\n",
    "print(x_stk_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KFold 0 CV Score: 1.0\n",
      "KFold 1 CV Score: 1.0\n",
      "KFold 2 CV Score: 1.0\n",
      "KFold 3 CV Score: 1.0\n",
      "KFold 4 CV Score: 1.0\n",
      "KFold Mean CV Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "cl_stk_lr = LogisticRegression(solver='lbfgs', n_jobs=-1)\n",
    "cv_s = []\n",
    "for i, (tr_idx, cv_idx) in enumerate(KFold(n_splits=5, shuffle=True).split(x_stk_tr)):\n",
    "    cl_stk_lr.fit(x_stk_tr[tr_idx], y_tr[tr_idx])\n",
    "    cv_s.append(f1_score(y_tr[cv_idx], cl_stk_lr.predict(x_stk_tr[cv_idx])))\n",
    "    logger.info('KFold {} CV Score: {}'.format(i, cv_s[-1]))\n",
    "logger.info('KFold Mean CV Score: {}'.format(sum(cv_s) / 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cl_stk_lr.fit(x_stk_tr, y_tr)\n",
    "with open('./results/stacking.csv', 'w') as f:\n",
    "    f.write('Id,Category\\n')\n",
    "    prd = cl_stk_lr.predict(x_stk_ts)\n",
    "    for i, y_i in enumerate(prd):\n",
    "        f.write('{i},{y_i}\\n'.format(i=i, y_i=int(y_i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Only run this to generate the info gain csv\n",
    "\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# ig_path = './files/info_gain.csv'\n",
    "# mi = mutual_info_classif(x_tr, y_tr)\n",
    "# with open(ig_path, 'w') as f:\n",
    "#     for mi_i, fn_i in sorted(zip(mi, fn), key=lambda x: -x[0]):\n",
    "#         f.write('{mi_i},{fn_i}\\n'.format(mi_i=mi_i, fn_i=fn_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
