{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "np.random.seed(2019)\n",
    "\n",
    "log_path = datetime.now().strftime('./logs/%Y-%m-%d-%H-%M-%S.log')\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-18 11:03:35.583870] Start\n",
      "[2019-02-18 11:03:48.168251] Finish\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_data(dp):\n",
    "    x = []\n",
    "    for fn in sorted(os.listdir(dp), key=lambda y: int(y[:-4])):\n",
    "        with open('{dp}{fn}'.format(dp=dp, fn=fn), 'r') as f:\n",
    "            x.append(f.read())\n",
    "    return x\n",
    "\n",
    "logger.info('[{t}] Start'.format(t=datetime.now()))\n",
    "\n",
    "x_tr_pos = np.array(load_data('./dataset/train/pos/'), dtype=np.str)\n",
    "x_tr_neg = np.array(load_data('./dataset/train/neg/'), dtype=np.str)\n",
    "x_tr = np.concatenate((x_tr_pos, x_tr_neg), axis=0)\n",
    "y_tr = np.concatenate((np.ones_like(x_tr_pos, dtype=np.float64), np.zeros_like(x_tr_neg, dtype=np.float64)), axis=0)\n",
    "x_ts = np.array(load_data('./dataset/test/'), dtype=np.str)\n",
    "\n",
    "del x_tr_pos\n",
    "del x_tr_neg\n",
    "\n",
    "logger.info('[{t}] Finish'.format(t=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocessor(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace('<br /><br />', ' ')\n",
    "    s = s.replace('-', ' ')\n",
    "    s = s.replace('/', ' ')\n",
    "    for ws in string.whitespace:\n",
    "        s.replace(ws, ' ')\n",
    "    s = s.translate(s.maketrans('', '', string.punctuation))\n",
    "    s = s.translate(s.maketrans('', '', string.digits))\n",
    "    s = ''.join(filter(lambda x: x in string.printable, s))\n",
    "    return s\n",
    "\n",
    "\n",
    "def tokenizer(s):\n",
    "    wl = WordNetLemmatizer()\n",
    "    st = SnowballStemmer('english', ignore_stopwords=True)\n",
    "    ts = word_tokenize(s, 'english')\n",
    "    ts = list(filter(lambda x: x not in stopwords.words('english'), ts))\n",
    "    ts = list(map(lambda x: wl.lemmatize(x), ts))\n",
    "    ts = list(map(lambda x: st.stem(x), ts))\n",
    "    return ts\n",
    "\n",
    "token_pattern = r'\\w+|[%s]' % string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ig_path = './files/info_gain.csv'\n",
    "vocabulary = None\n",
    "if os.path.exists(ig_path):\n",
    "    vocabulary = np.genfromtxt(ig_path, delimiter=',', dtype=np.str)[:, 1].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer(preprocessor=preprocessor,\n",
    "                         tokenizer=tokenizer,\n",
    "                         ngram_range=(1, 3),\n",
    "                         vocabulary=vocabulary,\n",
    "                         binary=True,\n",
    "                         use_idf=False,\n",
    "                         smooth_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cnt = CountVectorizer(token_pattern=token_pattern,\n",
    "                      ngram_range=(1, 3),\n",
    "                      binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-18 11:03:54.227965] Start\n",
      "[2019-02-18 11:05:34.005563] Finish\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "logger.info('[{t}] Start'.format(t=datetime.now()))\n",
    "\n",
    "x_tr = cnt.fit_transform(x_tr)\n",
    "x_ts = cnt.transform(x_ts)\n",
    "fn = np.array(cnt.get_feature_names(), dtype=np.str)\n",
    "\n",
    "logger.info('[{t}] Finish'.format(t=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 4996192)\n",
      "(25000, 4996192)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.shape)\n",
    "print(x_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "def k_fold_cross_validation(k, cl, x_tr, y_tr):\n",
    "    cv = KFold(n_splits=k, shuffle=True)\n",
    "    cv_s = cross_validate(cl, x_tr, y_tr, cv=cv, scoring='f1', return_train_score=True, verbose=1, n_jobs=-1)\n",
    "    logger.info('KFold Cross Validation Scores: {cv_s}'.format(cv_s=cv_s, indent=4))\n",
    "    logger.info('Mean CV Error: {e}'.format(e=sum(cv_s['test_score']) / k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(cl, x_tr, y_tr, x_ts, fn):\n",
    "    cl.fit(x_tr, y_tr)\n",
    "    with open('./results/{fn}.csv'.format(fn=fn), 'w') as f:\n",
    "        f.write('Id,Category\\n')\n",
    "        for i, y_i in enumerate(cl.predict(x_ts)):\n",
    "            f.write('{i},{y_i}\\n'.format(i=i, y_i=int(y_i)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "cl_gnb = GaussianNB()\n",
    "k_fold_cross_validation(4, clone(cl_gnb), x_tr.toarray(), y_tr)\n",
    "predict_test(cl_gnb, x_tr.toarray(), y_tr, x_ts.toarray(), 'gaussian_naive_bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "cl_bnb = BernoulliNB()\n",
    "k_fold_cross_validation(4, clone(cl_bnb), x_tr.toarray(), y_tr)\n",
    "predict_test(cl_bnb, x_tr.toarray(), y_tr, x_ts.toarray(), 'bernoulli_naive_bayes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cl_lr = LogisticRegression(solver='lbfgs', verbose=1, n_jobs=-1, C=1)\n",
    "k_fold_cross_validation(4, clone(cl_lr), x_tr, y_tr)\n",
    "predict_test(cl_lr, x_tr, y_tr, x_ts, 'logistic_regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "cl_ls = LinearSVC(C=0.75, verbose=1)\n",
    "k_fold_cross_validation(4, clone(cl_ls), x_tr, y_tr)\n",
    "predict_test(cl_ls, x_tr, y_tr, x_ts, 'linear_svc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "kc = KNeighborsClassifier(11, n_jobs=-1)\n",
    "k_fold_cross_validation(4, clone(kc), x_tr, y_tr)\n",
    "predict_test(kc, x_tr, y_tr, x_ts, 'knn_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:  4.6min remaining:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  4.7min finished\n",
      "KFold Cross Validation Scores: {'fit_time': array([277.31083679, 277.61475086, 266.52206779, 271.23024178]), 'score_time': array([0.10443807, 0.11288118, 0.24634409, 0.37811899]), 'test_score': array([0.92029216, 0.91406625, 0.91956591, 0.92249047]), 'train_score': array([1., 1., 1., 1.])}\n",
      "Mean CV Error: 0.919103697061775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from nbsvm import NBSVM\n",
    "\n",
    "cl_nb_svm = NBSVM()\n",
    "k_fold_cross_validation(4, clone(cl_nb_svm), x_tr, y_tr)\n",
    "predict_test(cl_nb_svm, x_tr, y_tr, x_ts, 'nb_svm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Only run this to regenerate the info gain csv\n",
    "\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "\n",
    "# logger.info('[{t}] Start'.format(t=datetime.now()))\n",
    "\n",
    "# mi = mutual_info_classif(x_tr, y_tr)\n",
    "# with open(ig_path, 'w') as f:\n",
    "#     for mi_i, fn_i in sorted(zip(mi, fn), key=lambda x: -x[0]):\n",
    "#         f.write('{mi_i},{fn_i}\\n'.format(mi_i=mi_i, fn_i=fn_i))\n",
    "\n",
    "# logger.info('[{t}] Finish'.format(t=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
