{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "np.random.seed(2019)\n",
    "\n",
    "log_path = datetime.now().strftime('./logs/%Y-%m-%d-%H-%M-%S.log')\n",
    "logging.basicConfig(filename=log_path, level=logging.INFO)\n",
    "logging.getLogger().addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 10:00:39.045885] Start\n",
      "[2019-02-15 10:00:51.840640] Finish\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_data(dp):\n",
    "    x = []\n",
    "    for fn in sorted(os.listdir(dp), key=lambda y: int(y[:-4])):\n",
    "        with open('{dp}{fn}'.format(dp=dp, fn=fn), 'r') as f:\n",
    "            x.append(f.read())\n",
    "    return x\n",
    "\n",
    "logger.info('[{t}] Start'.format(t=datetime.now()))\n",
    "\n",
    "x_tr_pos = np.array(load_data('./dataset/train/pos/'), dtype=np.str)\n",
    "x_tr_neg = np.array(load_data('./dataset/train/neg/'), dtype=np.str)\n",
    "x_tr = np.concatenate((x_tr_pos, x_tr_neg), axis=0)\n",
    "y_tr = np.concatenate((np.ones_like(x_tr_pos, dtype=np.float64), np.zeros_like(x_tr_neg, dtype=np.float64)), axis=0)\n",
    "x_ts = np.array(load_data('./dataset/test/'), dtype=np.str)\n",
    "\n",
    "del x_tr_pos\n",
    "del x_tr_neg\n",
    "\n",
    "logger.info('[{t}] Finish'.format(t=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def preprocessor(s):\n",
    "    s = s.lower()\n",
    "    s = s.replace('<br /><br />', ' ')\n",
    "    s = s.replace('-', ' ')\n",
    "    s = s.replace('/', ' ')\n",
    "    for ws in string.whitespace:\n",
    "        s.replace(ws, ' ')\n",
    "    s = s.translate(s.maketrans('', '', string.punctuation))\n",
    "    s = s.translate(s.maketrans('', '', string.digits))\n",
    "    s = ''.join(filter(lambda x: x in string.printable, s))\n",
    "    return s\n",
    "\n",
    "\n",
    "def tokenizer(s):\n",
    "    wl = WordNetLemmatizer()\n",
    "    st = SnowballStemmer('english', ignore_stopwords=True)\n",
    "    ts = word_tokenize(s, 'english')\n",
    "    ts = list(filter(lambda x: x not in stopwords.words('english'), ts))\n",
    "    ts = list(map(lambda x: wl.lemmatize(x), ts))\n",
    "    ts = list(map(lambda x: st.stem(x), ts))\n",
    "    return ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig_path = './files/info_gain.csv'\n",
    "vocabulary = None\n",
    "if os.path.exists(ig_path):\n",
    "    vocabulary = np.genfromtxt(ig_path, delimiter=',', dtype=np.str)[:, 1].squeeze()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 10:01:56.824954] Start\n",
      "[2019-02-15 10:46:03.877464] Finish\n"
     ]
    }
   ],
   "source": [
    "logger.info('[{t}] Start'.format(t=datetime.now()))\n",
    "\n",
    "tf_idf = TfidfVectorizer(preprocessor=preprocessor,\n",
    "                         tokenizer=tokenizer,\n",
    "                         ngram_range=(1, 3),\n",
    "                         vocabulary=vocabulary,\n",
    "                         binary=False,\n",
    "                         use_idf=True,\n",
    "                         smooth_idf=True)\n",
    "x_tr = tf_idf.fit_transform(x_tr)\n",
    "x_ts = tf_idf.transform(x_ts)\n",
    "fn = np.array(tf_idf.get_feature_names(), dtype=np.str)\n",
    "\n",
    "logger.info('[{t}] Finish'.format(t=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 10000)\n",
      "(25000, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(x_tr.shape)\n",
    "print(x_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate, KFold\n",
    "\n",
    "def k_fold_cross_validation(k, cl, x_tr, y_tr):\n",
    "    cv = KFold(n_splits=k, shuffle=True)\n",
    "    cv_s = cross_validate(cl, x_tr, y_tr, cv=cv, scoring='f1', return_train_score=True, verbose=1, n_jobs=-1)\n",
    "    logger.info('KFold Cross Validation Scores: {cv_s}'.format(cv_s=cv_s, indent=4))\n",
    "    return cv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test(cl, x_tr, y_tr, x_ts, fn):\n",
    "    cl.fit(x_tr, y_tr)\n",
    "    with open('./results/{fn}.csv'.format(fn=fn), 'w') as f:\n",
    "        f.write('Id,Category\\n')\n",
    "        for i, y_i in enumerate(cl.predict(x_ts)):\n",
    "            f.write('{i},{y_i}\\n'.format(i=i, y_i=int(y_i)))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:  4.6min remaining:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  4.6min finished\n",
      "KFold Cross Validation Scores: {'fit_time': array([115.14102101, 129.2283709 , 129.31780195, 129.3186481 ]), 'score_time': array([ 4.13920498, 14.288203  , 14.2076571 , 14.29917502]), 'test_score': array([0.83143259, 0.84015224, 0.82800318, 0.83338613]), 'train_score': array([0.89296797, 0.891862  , 0.89334318, 0.89461235])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8332435332181538\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "cl = GaussianNB()\n",
    "ss = k_fold_cross_validation(4, clone(cl), x_tr.toarray(), y_tr)\n",
    "predict_test(clone(cl), x_tr.toarray(), y_tr, x_ts.toarray(), 'gaussian_naive_bayes')\n",
    "print(sum(ss['test_score']) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:  1.9min remaining:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  1.9min finished\n",
      "KFold Cross Validation Scores: {'fit_time': array([71.78296924, 72.02037883, 72.51355004, 71.86622286]), 'score_time': array([1.91542864, 3.89467216, 4.20052791, 4.02848697]), 'test_score': array([0.860799  , 0.86700284, 0.86404548, 0.86380098]), 'train_score': array([0.88330794, 0.88048986, 0.8807561 , 0.88122605])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8639120724956857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "cl = BernoulliNB()\n",
    "ss = k_fold_cross_validation(4, clone(cl), x_tr.toarray(), y_tr)\n",
    "predict_test(clone(cl), x_tr.toarray(), y_tr, x_ts.toarray(), 'bernoulli_naive_bayes')\n",
    "print(sum(ss['test_score']) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    1.4s remaining:    1.4s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    1.5s finished\n",
      "KFold Cross Validation Scores: {'fit_time': array([1.42756796, 1.20593405, 1.3495729 , 1.32562089]), 'score_time': array([0.00543499, 0.00838923, 0.00443697, 0.00358105]), 'test_score': array([0.88928227, 0.88130285, 0.88825215, 0.89872234]), 'train_score': array([0.93598907, 0.93716889, 0.93579047, 0.93263035])}\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8893899020604852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cl = LogisticRegression(solver='lbfgs', verbose=1, n_jobs=-1, C=1.5)\n",
    "ss = k_fold_cross_validation(4, clone(cl), x_tr, y_tr)\n",
    "predict_test(clone(cl), x_tr, y_tr, x_ts, 'logistic_regression')\n",
    "print(sum(ss['test_score']) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   4 | elapsed:    0.5s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:    0.5s finished\n",
      "KFold Cross Validation Scores: {'fit_time': array([0.37445402, 0.38459802, 0.371243  , 0.37494302]), 'score_time': array([0.00579715, 0.00595498, 0.00578403, 0.0034399 ]), 'test_score': array([0.89269442, 0.88917361, 0.89112333, 0.8884694 ]), 'train_score': array([0.94287076, 0.94187701, 0.94535984, 0.94552013])}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]0.8903651909519483\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "cl = LinearSVC(C=0.2, verbose=1)\n",
    "ss = k_fold_cross_validation(4, clone(cl), x_tr, y_tr)\n",
    "predict_test(clone(cl), x_tr, y_tr, x_ts, 'linear_svc')\n",
    "print(sum(ss['test_score']) / 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Only run this to regenerate the info gain csv\n",
    "\n",
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "\n",
    "# logger.info('[{t}] Start'.format(t=datetime.now()))\n",
    "\n",
    "# mi = mutual_info_classif(x_tr, y_tr)\n",
    "# with open(ig_path, 'w') as f:\n",
    "#     for mi_i, fn_i in sorted(zip(mi, fn), key=lambda x: -x[0]):\n",
    "#         f.write('{mi_i},{fn_i}\\n'.format(mi_i=mi_i, fn_i=fn_i))\n",
    "\n",
    "# logger.info('[{t}] Finish'.format(t=datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
